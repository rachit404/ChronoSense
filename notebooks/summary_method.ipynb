{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b30e8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating and demonstrating the requested functions for precomputed time-series analysis.\n",
    "# This code will:\n",
    "# 1. Define load_data(file_path, col, date_col='Date') that loads the CSV and prepares a DataFrame.\n",
    "# 2. Define precompute_basic_features(df, price_col, windows=[7,30], rsi_window=14) that computes:\n",
    "#    - rolling mean, median, std for the windows\n",
    "#    - daily returns and cumulative returns\n",
    "#    - SMA (rolling mean) and EMA\n",
    "#    - rolling correlation with Volume (if present)\n",
    "#    - volatility (std of returns)\n",
    "#    - RSI and MACD\n",
    "# 3. Define save_pre_analysis(df, original_path) to save to a new CSV with suffix \"_pre_analysis\".\n",
    "# 4. Demonstrate running these functions on /mnt/data/goog_stock_data.csv using 'Close' if present.\n",
    "# Note: this environment has no internet access. This will run locally and produce a CSV in /mnt/data.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from typing import List\n",
    "\n",
    "def load_data(file_path: str, col: str, date_col: str = \"Date\", parse_dates: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load CSV into a pandas DataFrame, set the date column as a datetime index (if present),\n",
    "    and ensure the requested column exists.\n",
    "    \n",
    "    Parameters\n",
    "    ---------\n",
    "    file_path : str\n",
    "        Path to the CSV file.\n",
    "    col : str\n",
    "        Name of the price column to focus on (e.g., 'Close').\n",
    "    date_col : str\n",
    "        Name of the date column. Defaults to 'Date'.\n",
    "    parse_dates : bool\n",
    "        Whether to parse the date column as datetime.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Loaded DataFrame with a datetime index (if date_col exists) and containing the requested column.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Date\"], utc=True)\n",
    "    df = df.set_index(\"Date\").sort_index()\n",
    "    # parse date column and set index if exists\n",
    "    if parse_dates and date_col in df.columns:\n",
    "        df[date_col] = pd.to_datetime(df[date_col], errors=\"coerce\")\n",
    "        if df[date_col].isnull().all():\n",
    "            # failed parsing, keep as-is\n",
    "            df = df.copy()\n",
    "        else:\n",
    "            df = df.set_index(date_col).sort_index()\n",
    "    # ensure the requested column exists; if not, try to pick a sensible default\n",
    "    if col not in df.columns:\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        fallback = numeric_cols[0] if numeric_cols else None\n",
    "        raise ValueError(f\"Column '{col}' not found in file. Numeric columns available: {numeric_cols}. \"\n",
    "                         f\"Consider using one of those (e.g. '{fallback}').\")\n",
    "    return df\n",
    "\n",
    "def _ema(series: pd.Series, span: int) -> pd.Series:\n",
    "    return series.ewm(span=span, adjust=False).mean()\n",
    "\n",
    "def precompute_basic_features(df: pd.DataFrame, price_col: str,\n",
    "                              windows: List[int] = [7, 30],\n",
    "                              rsi_window: int = 14) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute basic statistical/time-series features described by the user:\n",
    "      - rolling mean, median, std for windows\n",
    "      - daily returns and cumulative returns\n",
    "      - SMA (rolling mean) and EMA\n",
    "      - rolling correlations with Volume if present\n",
    "      - volatility (std of returns)\n",
    "      - RSI and MACD\n",
    "    \n",
    "    Returns a new DataFrame with added columns.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    price = df[price_col].astype(float)\n",
    "    \n",
    "    # Daily returns\n",
    "    df['daily_return'] = price.pct_change()\n",
    "    # Cumulative return from start\n",
    "    df['cumulative_return'] = (1 + df['daily_return']).cumprod() - 1\n",
    "\n",
    "    # Rolling stats and MAs/EMAs\n",
    "    for w in windows:\n",
    "        df[f'rolling_mean_{w}'] = price.rolling(window=w, min_periods=1).mean()\n",
    "        df[f'rolling_median_{w}'] = price.rolling(window=w, min_periods=1).median()\n",
    "        df[f'rolling_std_{w}'] = price.rolling(window=w, min_periods=1).std(ddof=0)\n",
    "        # EMA and SMA (SMA = rolling mean)\n",
    "        df[f'ema_{w}'] = _ema(price, span=w)\n",
    "        df[f'sma_{w}'] = df[f'rolling_mean_{w}']\n",
    "\n",
    "    # Volatility: std of returns over windows\n",
    "    for w in windows:\n",
    "        df[f'volatility_{w}'] = df['daily_return'].rolling(window=w, min_periods=1).std(ddof=0)\n",
    "\n",
    "    # Rolling correlation with volume if volume exists\n",
    "    if 'Volume' in df.columns:\n",
    "        for w in windows:\n",
    "            df[f'rolling_corr_price_volume_{w}'] = price.rolling(window=w, min_periods=1).corr(df['Volume'])\n",
    "\n",
    "    # RSI implementation (Wilder's smoothing)\n",
    "    delta = price.diff()\n",
    "    up = delta.clip(lower=0.0)\n",
    "    down = -1 * delta.clip(upper=0.0)\n",
    "    # Use exponential moving average of gains/losses (Wilder's)\n",
    "    roll_up = up.ewm(alpha=1/rsi_window, adjust=False, min_periods=1).mean()\n",
    "    roll_down = down.ewm(alpha=1/rsi_window, adjust=False, min_periods=1).mean()\n",
    "    rs = roll_up / (roll_down.replace(0, np.nan))\n",
    "    df['rsi'] = 100 - (100 / (1 + rs))\n",
    "    df['rsi'] = df['rsi'].fillna(0)\n",
    "\n",
    "    # MACD: EMA12 - EMA26 and signal line 9-day EMA of MACD\n",
    "    ema_short = _ema(price, span=12)\n",
    "    ema_long = _ema(price, span=26)\n",
    "    df['macd'] = ema_short - ema_long\n",
    "    df['macd_signal'] = _ema(df['macd'].fillna(0), span=9)\n",
    "    df['macd_histogram'] = df['macd'] - df['macd_signal']\n",
    "    \n",
    "    # Momentum: difference between current price and price n periods ago (we'll include for windows)\n",
    "    for w in windows:\n",
    "        df[f'momentum_{w}'] = price - price.shift(w)\n",
    "    \n",
    "    # Clean up infinite values if any and keep numeric columns consistent\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def save_pre_analysis(df: pd.DataFrame, original_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Save dataframe to a new CSV with suffix \"_pre_analysis\" before the file extension.\n",
    "    Returns the path to the saved CSV.\n",
    "    \"\"\"\n",
    "    base, ext = os.path.splitext(original_path)\n",
    "    new_path = f\"{base}_pre_analysis{ext}\"\n",
    "    df.to_csv(new_path, index=True)\n",
    "    return new_path\n",
    "\n",
    "# --- Demonstration on the provided file (if present) ---\n",
    "# input_path = \"/mnt/data/goog_stock_data.csv\"\n",
    "# demo_output_path = None\n",
    "# try:\n",
    "#     # Try to load using 'Close' as the price column; if not present, pick a numeric column automatically.\n",
    "#     try:\n",
    "#         demo_df = load_data(input_path, col='Close', date_col='Date')\n",
    "#         chosen_col = 'Close'\n",
    "#     except ValueError as e:\n",
    "#         # If 'Close' not present, inspect columns and choose first numeric column\n",
    "#         tmp = pd.read_csv(input_path)\n",
    "#         numeric_cols = tmp.select_dtypes(include=[np.number]).columns.tolist()\n",
    "#         if not numeric_cols:\n",
    "#             raise RuntimeError(\"No numeric columns found in the uploaded CSV; cannot compute time-series features.\")\n",
    "#         chosen_col = numeric_cols[-1]  # choose last numeric column as fallback\n",
    "#         demo_df = load_data(input_path, col=chosen_col, date_col='Date')\n",
    "    \n",
    "#     analyzed = precompute_basic_features(demo_df, price_col=chosen_col, windows=[7, 30], rsi_window=14)\n",
    "#     demo_output_path = save_pre_analysis(analyzed, input_path)\n",
    "#     print(\"✅ Pre-analysis completed.\")\n",
    "#     print(f\"Input file: {input_path}\")\n",
    "#     print(f\"Using price column: {chosen_col}\")\n",
    "#     print(f\"Saved output to: {demo_output_path}\")\n",
    "#     # show top rows\n",
    "#     analyzed.head(8)\n",
    "# except Exception as e:\n",
    "#     print(\"❌ Error during demonstration:\", str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b64bdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from langchain_core.documents import Document\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# -------------------------\n",
    "# B. Trend & Seasonality\n",
    "# -------------------------\n",
    "def compute_trend_seasonality(df: pd.DataFrame, price_col: str, model: str = 'additive', period: int = 30) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df = df.dropna(subset=[price_col])\n",
    "    result = seasonal_decompose(df[price_col], model=model, period=period, extrapolate_trend='freq')\n",
    "    df['trend'] = result.trend\n",
    "    df['seasonal'] = result.seasonal\n",
    "    df['residual'] = result.resid\n",
    "    return df\n",
    "\n",
    "# -------------------------\n",
    "# C. Anomaly Detection\n",
    "# -------------------------\n",
    "def detect_anomalies(df: pd.DataFrame, price_col: str, z_thresh: float = 3.0) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    # Z-score method\n",
    "    df['zscore'] = (df[price_col] - df[price_col].mean()) / df[price_col].std(ddof=0)\n",
    "    df['zscore_anomaly'] = df['zscore'].abs() > z_thresh\n",
    "\n",
    "    # Isolation Forest\n",
    "    scaler = StandardScaler()\n",
    "    scaled_vals = scaler.fit_transform(df[[price_col]].fillna(0))\n",
    "    iso = IsolationForest(contamination=0.01, random_state=42)\n",
    "    df['isolation_anomaly'] = iso.fit_predict(scaled_vals) == -1\n",
    "    return df\n",
    "\n",
    "# -------------------------\n",
    "# D. Forecasting\n",
    "# -------------------------\n",
    "def forecast_arima(df: pd.DataFrame, price_col: str, steps: int = 7):\n",
    "    df = df.dropna(subset=[price_col])\n",
    "    model = ARIMA(df[price_col], order=(5,1,0))\n",
    "    model_fit = model.fit()\n",
    "    forecast = model_fit.forecast(steps=steps)\n",
    "    forecast_df = pd.DataFrame({\n",
    "        'forecast_date': pd.date_range(df.index[-1] + pd.Timedelta(days=1), periods=steps),\n",
    "        'forecast_arima': forecast.values\n",
    "    })\n",
    "    return forecast_df\n",
    "\n",
    "def forecast_lstm(df: pd.DataFrame, price_col: str, steps: int = 7, epochs: int = 5):\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from numpy import array\n",
    "\n",
    "    prices = df[price_col].dropna().values.reshape(-1, 1)\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled = scaler.fit_transform(prices)\n",
    "\n",
    "    X, y = [], []\n",
    "    for i in range(len(scaled)-1):\n",
    "        X.append(scaled[i])\n",
    "        y.append(scaled[i+1])\n",
    "    X, y = np.array(X), np.array(y)\n",
    "    X = X.reshape((X.shape[0], 1, X.shape[1]))\n",
    "\n",
    "    model = Sequential([\n",
    "        LSTM(50, activation='relu', input_shape=(1,1)),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    model.fit(X, y, epochs=epochs, verbose=0)\n",
    "\n",
    "    # forecast\n",
    "    last_val = scaled[-1].reshape((1,1,1))\n",
    "    preds = []\n",
    "    for _ in range(steps):\n",
    "        next_pred = model.predict(last_val, verbose=0)\n",
    "        preds.append(next_pred[0,0])\n",
    "        last_val = next_pred.reshape((1,1,1))\n",
    "    preds = scaler.inverse_transform(np.array(preds).reshape(-1,1)).flatten()\n",
    "    forecast_df = pd.DataFrame({\n",
    "        'forecast_date': pd.date_range(df.index[-1] + pd.Timedelta(days=1), periods=steps),\n",
    "        'forecast_lstm': preds\n",
    "    })\n",
    "    return forecast_df\n",
    "\n",
    "# -------------------------\n",
    "# E. Summary for Embedding\n",
    "# -------------------------\n",
    "def create_langchain_summaries(df: pd.DataFrame, price_col: str, freq: str = 'M') -> list:\n",
    "    \"\"\"\n",
    "    Create multiple LangChain Documents summarizing trends/anomalies per time chunk (e.g., month).\n",
    "    \"\"\"\n",
    "    docs = []\n",
    "    \n",
    "    # ✅ Ensure the DataFrame has a DatetimeIndex\n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        # Try to auto-detect a date column\n",
    "        date_cols = [c for c in df.columns if 'date' in c.lower()]\n",
    "        if date_cols:\n",
    "            df[date_cols[0]] = pd.to_datetime(df[date_cols[0]], errors='coerce')\n",
    "            df = df.set_index(date_cols[0])\n",
    "        else:\n",
    "            raise ValueError(\"No DatetimeIndex or 'Date' column found for resampling.\")\n",
    "    \n",
    "    grouped = df.resample(freq)\n",
    "    for period, group in grouped:\n",
    "        if len(group) < 5:\n",
    "            continue\n",
    "        mean_price = group[price_col].mean()\n",
    "        vol = group['daily_return'].std() if 'daily_return' in group else group[price_col].pct_change().std()\n",
    "        anomalies = group['zscore_anomaly'].sum() if 'zscore_anomaly' in group else 0\n",
    "        trend_desc = \"increasing\" if group['trend'].iloc[-1] > group['trend'].iloc[0] else \"decreasing\"\n",
    "\n",
    "        text = (\n",
    "            f\"Between {group.index.min().date()} and {group.index.max().date()}, \"\n",
    "            f\"the average {price_col} was {mean_price:.2f}. \"\n",
    "            f\"The overall trend was {trend_desc}. \"\n",
    "            f\"Volatility measured by return std was {vol:.4f}. \"\n",
    "            f\"{anomalies} anomalies were detected this period.\"\n",
    "        )\n",
    "\n",
    "        meta = {\n",
    "            \"period\": str(period.date()),\n",
    "            \"mean_price\": mean_price,\n",
    "            \"volatility\": vol,\n",
    "            \"num_anomalies\": int(anomalies)\n",
    "        }\n",
    "        docs.append(Document(page_content=text, metadata=meta))\n",
    "    return docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "471cf2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatetimeIndex(['2020-10-26 04:00:00+00:00', '2020-10-27 04:00:00+00:00',\n",
      "               '2020-10-28 04:00:00+00:00', '2020-10-29 04:00:00+00:00',\n",
      "               '2020-10-30 04:00:00+00:00', '2020-11-02 05:00:00+00:00',\n",
      "               '2020-11-03 05:00:00+00:00', '2020-11-04 05:00:00+00:00',\n",
      "               '2020-11-05 05:00:00+00:00', '2020-11-06 05:00:00+00:00',\n",
      "               ...\n",
      "               '2025-10-13 04:00:00+00:00', '2025-10-14 04:00:00+00:00',\n",
      "               '2025-10-15 04:00:00+00:00', '2025-10-16 04:00:00+00:00',\n",
      "               '2025-10-17 04:00:00+00:00', '2025-10-20 04:00:00+00:00',\n",
      "               '2025-10-21 04:00:00+00:00', '2025-10-22 04:00:00+00:00',\n",
      "               '2025-10-23 04:00:00+00:00', '2025-10-24 04:00:00+00:00'],\n",
      "              dtype='datetime64[ns, UTC]', name='Date', length=1256, freq=None)\n",
      "Between 2020-10-26 and 2020-10-30, the average Close was 78.46. The overall trend was increasing. Volatility measured by return std was 0.0418. 0 anomalies were detected this period.\n",
      "{'period': '2020-10-31', 'mean_price': np.float64(78.4588851928711), 'volatility': np.float64(0.04176420099595921), 'num_anomalies': 0}\n"
     ]
    }
   ],
   "source": [
    "col = \"Close\"\n",
    "df = load_data(\"../data/goog_stock_data.csv\", col=\"Close\")\n",
    "print(df.index)\n",
    "df = precompute_basic_features(df, \"Close\")\n",
    "\n",
    "# B & C\n",
    "df = compute_trend_seasonality(df, \"Close\")\n",
    "df = detect_anomalies(df, \"Close\")\n",
    "\n",
    "# D\n",
    "arima_forecast = forecast_arima(df, \"Close\")\n",
    "lstm_forecast = forecast_lstm(df, \"Close\", epochs=5)\n",
    "\n",
    "# Merge forecasts if needed\n",
    "df_forecasts = pd.merge(arima_forecast, lstm_forecast, on=\"forecast_date\", how=\"outer\")\n",
    "\n",
    "# E\n",
    "docs = create_langchain_summaries(df, \"Close\")\n",
    "print(docs[0].page_content)\n",
    "print(docs[0].metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3b46e6c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded embedding model: all-MiniLM-L6-v2 on cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  4.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Connected to ChromaDB collection: goog_timeseries\n",
      "🧹 Collection already empty.\n",
      "📚 Added 61 documents to ChromaDB\n",
      "\n",
      "🧩 Match:\n",
      "Text: Between 2022-06-01 and 2022-06-30, the average Close was 112.06. The overall trend was increasing. Volatility measured by return std was 0.0261. 0 anomalies were detected this period.\n",
      "Score: 0.3013\n",
      "Metadata: {'mean_price': 112.05827476864768, 'num_anomalies': 0, 'volatility': 0.026124593312125684, 'period': '2022-06-30'}\n",
      "\n",
      "🧩 Match:\n",
      "Text: Between 2025-01-02 and 2025-01-31, the average Close was 196.20. The overall trend was decreasing. Volatility measured by return std was 0.0167. 0 anomalies were detected this period.\n",
      "Score: 0.3077\n",
      "Metadata: {'period': '2025-01-31', 'volatility': 0.016701299623074674, 'mean_price': 196.19761352539064, 'num_anomalies': 0}\n",
      "\n",
      "🧩 Match:\n",
      "Text: Between 2022-12-01 and 2022-12-30, the average Close was 92.10. The overall trend was decreasing. Volatility measured by return std was 0.0173. 0 anomalies were detected this period.\n",
      "Score: 0.3114\n",
      "Metadata: {'volatility': 0.017334005157809407, 'num_anomalies': 0, 'mean_price': 92.103519984654, 'period': '2022-12-31'}\n"
     ]
    }
   ],
   "source": [
    "# ==============================================\n",
    "# vector_pipeline.py\n",
    "# ==============================================\n",
    "\n",
    "from typing import List, Union\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 1️⃣ Embedding Engine\n",
    "# -------------------------------------------------\n",
    "class EmbeddingEngine:\n",
    "    \"\"\"\n",
    "    A wrapper around SentenceTransformer for encoding LangChain Documents.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\", device: str = \"cpu\"):\n",
    "        self.model_name = model_name\n",
    "        self.device = device\n",
    "        self.model = SentenceTransformer(model_name, device=device)\n",
    "        print(f\"✅ Loaded embedding model: {model_name} on {device}\")\n",
    "\n",
    "    def embed_documents(self, docs: List[Document]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Encodes a list of LangChain Document objects into dense vectors.\n",
    "        \"\"\"\n",
    "        texts = [doc.page_content for doc in docs]\n",
    "        embeddings = self.model.encode(texts, batch_size=32, show_progress_bar=True, convert_to_numpy=True)\n",
    "        return embeddings\n",
    "\n",
    "    def embed_query(self, query: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Encodes a single query string into a dense vector.\n",
    "        \"\"\"\n",
    "        return self.model.encode([query], convert_to_numpy=True)[0]\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 2️⃣ Vector Store (ChromaDB)\n",
    "# -------------------------------------------------\n",
    "class VectorStore:\n",
    "    \"\"\"\n",
    "    A persistent vector store using ChromaDB.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, persist_dir: str = \"./chroma_store\", collection_name: str = \"timeseries_docs\", model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        self.client = chromadb.PersistentClient(path=persist_dir)\n",
    "        self.model_name = model_name\n",
    "        self.embedding_fn = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=model_name)\n",
    "        self.collection = self.client.get_or_create_collection(\n",
    "            name=collection_name,\n",
    "            embedding_function=self.embedding_fn\n",
    "        )\n",
    "        print(f\"✅ Connected to ChromaDB collection: {collection_name}\")\n",
    "\n",
    "    def add_documents(self, docs: List[Document], embeddings: np.ndarray):\n",
    "        \"\"\"\n",
    "        Adds documents + embeddings to Chroma collection.\n",
    "        \"\"\"\n",
    "        ids = [f\"doc_{i}\" for i in range(len(docs))]\n",
    "        metadatas = [doc.metadata for doc in docs]\n",
    "        texts = [doc.page_content for doc in docs]\n",
    "        self.collection.add(documents=texts, embeddings=embeddings.tolist(), metadatas=metadatas, ids=ids)\n",
    "        print(f\"📚 Added {len(docs)} documents to ChromaDB\")\n",
    "\n",
    "    def query(self, query_embedding: np.ndarray, top_k: int = 3):\n",
    "        \"\"\"\n",
    "        Retrieves top-k similar documents given a query embedding.\n",
    "        \"\"\"\n",
    "        results = self.collection.query(query_embeddings=[query_embedding.tolist()], n_results=top_k)\n",
    "        hits = [\n",
    "            {\"text\": results[\"documents\"][0][i], \"score\": results[\"distances\"][0][i], \"metadata\": results[\"metadatas\"][0][i]}\n",
    "            for i in range(len(results[\"documents\"][0]))\n",
    "        ]\n",
    "        return hits\n",
    "\n",
    "    def clear(self):\n",
    "        \"\"\"Clears all documents in the ChromaDB collection safely.\"\"\"\n",
    "        all_ids = self.collection.get()[\"ids\"]\n",
    "        if all_ids:\n",
    "            self.collection.delete(ids=all_ids)\n",
    "            print(f\"🧹 Cleared {len(all_ids)} documents from ChromaDB collection.\")\n",
    "        else:\n",
    "            print(\"🧹 Collection already empty.\")\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 3️⃣ Example Pipeline Usage\n",
    "# -------------------------------------------------\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Assuming 'docs' is your list of LangChain Documents\n",
    "#     from my_pipeline import docs   # Replace this with your own import\n",
    "\n",
    "# Step 1: Embed\n",
    "embedder = EmbeddingEngine(model_name=\"all-MiniLM-L6-v2\")\n",
    "doc_embeddings = embedder.embed_documents(docs)\n",
    "\n",
    "# Step 2: Store in Chroma\n",
    "store = VectorStore(persist_dir=\"./chroma_store\", collection_name=\"goog_timeseries\")\n",
    "store.clear()\n",
    "store.add_documents(docs, doc_embeddings)\n",
    "\n",
    "    # Step 3: Query\n",
    "user_query = \"Explain the volatility trend for March 2024\"\n",
    "query_vec = embedder.embed_query(user_query)\n",
    "results = store.query(query_vec, top_k=3)\n",
    "\n",
    "for r in results:\n",
    "    print(\"\\n🧩 Match:\")\n",
    "    print(\"Text:\", r[\"text\"])\n",
    "    print(\"Score:\", round(r[\"score\"], 4))\n",
    "    print(\"Metadata:\", r[\"metadata\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c209e5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be413016",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47b334e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e114197",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e6fe95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95b399e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                Open       High        Low      Close  \\\n",
      "Date                                                                    \n",
      "2020-10-26 04:00:00+00:00  80.698262  81.355265  78.289241  78.982002   \n",
      "2020-10-27 04:00:00+00:00  79.241239  79.796187  78.601118  79.667816   \n",
      "2020-10-28 04:00:00+00:00  77.456942  77.536893  75.216279  75.315598   \n",
      "2020-10-29 04:00:00+00:00  75.600633  79.143890  75.594677  77.829384   \n",
      "2020-10-30 04:00:00+00:00  83.037261  83.776700  79.677751  80.499626   \n",
      "\n",
      "                             Volume  Dividends  Stock Splits  \n",
      "Date                                                          \n",
      "2020-10-26 04:00:00+00:00  37066000        0.0           0.0  \n",
      "2020-10-27 04:00:00+00:00  24580000        0.0           0.0  \n",
      "2020-10-28 04:00:00+00:00  36680000        0.0           0.0  \n",
      "2020-10-29 04:00:00+00:00  40062000        0.0           0.0  \n",
      "2020-10-30 04:00:00+00:00  86582000        0.0           0.0  \n",
      "Index(['Open', 'High', 'Low', 'Close', 'Volume', 'Dividends', 'Stock Splits'], dtype='object')\n",
      "Open            float64\n",
      "High            float64\n",
      "Low             float64\n",
      "Close           float64\n",
      "Volume            int64\n",
      "Dividends       float64\n",
      "Stock Splits    float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../data/goog_stock_data.csv\")\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"], utc=True)\n",
    "df = df.set_index(\"Date\").sort_index()\n",
    "print(df.head())\n",
    "print(df.columns)\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5bc09b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatetimeIndex(['2020-10-26 04:00:00+00:00', '2020-10-27 04:00:00+00:00',\n",
      "               '2020-10-28 04:00:00+00:00', '2020-10-29 04:00:00+00:00',\n",
      "               '2020-10-30 04:00:00+00:00', '2020-11-02 05:00:00+00:00',\n",
      "               '2020-11-03 05:00:00+00:00', '2020-11-04 05:00:00+00:00',\n",
      "               '2020-11-05 05:00:00+00:00', '2020-11-06 05:00:00+00:00',\n",
      "               ...\n",
      "               '2025-10-13 04:00:00+00:00', '2025-10-14 04:00:00+00:00',\n",
      "               '2025-10-15 04:00:00+00:00', '2025-10-16 04:00:00+00:00',\n",
      "               '2025-10-17 04:00:00+00:00', '2025-10-20 04:00:00+00:00',\n",
      "               '2025-10-21 04:00:00+00:00', '2025-10-22 04:00:00+00:00',\n",
      "               '2025-10-23 04:00:00+00:00', '2025-10-24 04:00:00+00:00'],\n",
      "              dtype='datetime64[ns, UTC]', name='Date', length=1256, freq=None)\n"
     ]
    }
   ],
   "source": [
    "print(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e3a0b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20327c09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ChronoSense",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
